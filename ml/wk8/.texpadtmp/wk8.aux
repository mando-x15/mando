\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\BKM@entry[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@ifundefined{amsrefs@bibcite}{}{\let\bibcite\amsrefs@bibcite}
\bibstyle{amsrn}
\BKM@entry{id=1,open,dest={73656374696F6E2E38},srcline={14}}{382E20556E73757065727669736564204C6561726E696E673A20436C7573746572696E67}
\BKM@entry{id=2,open,dest={73756273656374696F6E2E382E31},srcline={53}}{382E312E206B2D4D65616E7320436C7573746572696E67}
\providecommand\tcolorbox@label[2]{}
\providecommand\mph@setcol[2]{}
\@writefile{toc}{\contentsline {section}{\tocsection {}{8}{\relax $\@@underline {\hbox {Unsupervised Learning: Clustering}}\mathsurround \z@ $\relax }}{1}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{8.1}{$k$-Means Clustering}}{1}{subsection.8.1}}
\mph@setcol{ii:1}{\mph@nr}
\BKM@entry{id=3,open,dest={73756273756273656374696F6E2E382E312E31},srcline={118}}{382E312E312E206B2D6D65616E7320666F72206E6F6E2D73657061726174656420636C757374657273}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $k$-means clustering algorithm: Training examples are shown as dots, and cluster centroids are shown as crosses. \textbf  {(a)} Original dataset. \textbf  {(b)} Random initial cluster centroids \textit  {(in this instance, not chosen to be equal to two training examples)}. \textbf  {(c-f)} Illustration of running two iterations of k-means. In each iteration, we assign each training example to the closest cluster centroid (shown by \IeC {\textquotedblleft }painting\IeC {\textquotedblright } the training examples the same color as the cluster centroid to which is assigned); then we move each cluster centroid to the mean of the points assigned to it. (Best viewed in color.)\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{1}{2}{$k$-means clustering algorithm: Training examples are shown as dots, and cluster centroids are shown as crosses. \bd {(a)} Original dataset. \bd {(b)} Random initial cluster centroids \it {(in this instance, not chosen to be equal to two training examples)}. \bd {(c-f)} Illustration of running two iterations of k-means. In each iteration, we assign each training example to the closest cluster centroid (shown by “painting” the training examples the same color as the cluster centroid to which is assigned); then we move each cluster centroid to the mean of the points assigned to it. (Best viewed in color.)\relax }{figure.caption.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces k-means clustering\relax }}{2}{algocf.1}}
\mph@setcol{ii:2}{\mph@nr}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{17.77776pt}
\newlabel{tocindent2}{29.38896pt}
\newlabel{tocindent3}{0pt}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{8.1.1}{\textbf  {\textit  {k-means for non-separated clusters}}}}{3}{subsubsection.8.1.1}}
\mph@setcol{ii:3}{\mph@nr}
\gdef\mph@lastpage{4}
\csname mph@do@warn\endcsname
